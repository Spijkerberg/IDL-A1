Optimizer,Learning rate,Batch size,Epochs,Accuracy
Adamax,.0005,128,10,0.905
Adamax,0.001,256,10,0.903
Adamax,0.001,512,20,0.9
Adamax,.0005,64,10,0.898
Adam,0.001,128,10,0.896
Adam,0.001,256,10,0.896
Adamax,0.001,64,20,0.896
Nadam,0.001,128,5,0.895
Adamax,.0005,512,10,0.894
Adam,.0005,512,10,0.893
Nadam,0.001,256,5,0.892
Nadam,0.001,256,20,0.891
Adam,.0005,256,10,0.891
Adamax,.0005,256,10,0.89
Adamax,0.001,256,20,0.89
Adamax,0.001,128,10,0.89
Nadam,.0005,64,10,0.889
Nadam,0.001,64,20,0.889
Adam,0.001,256,5,0.887
Nadam,0.001,256,10,0.887
Adam,.0005,128,10,0.887
Adamax,0.001,128,20,0.887
Adam,0.001,512,5,0.887
Nadam,.0005,512,10,0.886
Adamax,0.001,64,5,0.886
Adam,.0005,64,10,0.885
Nadam,0.001,128,20,0.885
Adam,0.001,64,20,0.885
Nadam,.0005,128,10,0.883
Adam,0.001,256,20,0.883
Nadam,0.001,128,10,0.879
Adam,0.001,64,20,0.879
Adamax,0.001,128,5,0.879
Nadam,.0005,256,10,0.879
Nadam,0.001,64,5,0.877
Adam,0.001,512,20,0.875
Adam,0.001,64,5,0.873
Adam,0.001,64,30,0.873
Nadam,0.001,512,5,0.872
Adam,0.001,128,20,0.871
Adam,0.001,64,5,0.871
Adam,0.001,64,10,0.87
RMSprop,0.001,128,5,0.868
RMSprop,0.001,256,5,0.866
Adamax,0.001,256,5,0.866
Adamax,0.001,512,5,0.855
Adam,0.001,128,5,0.847
RMSprop,0.001,512,5,0.842
Adagrad,0.001,512,5,0.728
Adagrad,0.001,64,5,0.725
Adagrad,0.001,256,5,0.717
Adagrad,0.001,128,5,0.693
Adagrad,0.001,128,5,0.692
SGD,0.001,64,5,0.655
SGD,0.001,512,5,0.624
SGD,0.001,256,5,0.568
RMSprop,0.001,64,5,0.441
SGD,0.001,128,5,0.325
Adadelta,0.001,64,5,0.301
Adadelta,0.001,512,5,0.149
Adadelta,0.001,256,5,0.109
Adadelta,0.001,128,5,0.101
